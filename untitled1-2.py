# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ew1uFqTCN--dSsMdSXxiWIefbMe-duAC
"""

import os
import time
import json
import uuid
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# pip install -q streamlit pyngrok chromadb sentence-transformers transformers pypdf scikit-learn tiktoken joblib
# # quick check for torch version using the python in the environment
# python -c "import torch, sys; print('Torch:', getattr(torch, '__version__', 'not installed'))"
#

import streamlit as st

# --- Retrieval stack ---
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer

# --- NLP models ---
from transformers import pipeline

# --- Parsing ---
from pypdf import PdfReader

# --- ML ---
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline as SkPipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# --------- Config ---------
APP_NAME = "Customer Support RAG + Sentiment"
PERSIST_DIR = "./chroma_db"
COLLECTION_NAME = "kb_chunks"
DEFAULT_DOMAIN = "LegalTech SaaS Support"

# Light, fast models for demos
SENTIMENT_MODEL = "distilbert-base-uncased-finetuned-sst-2-english"
EMOTION_MODEL = "j-hartmann/emotion-english-distilroberta-base"  # will auto-download; can be heavy

# Embedding model (fast & widely available)
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# ---------------- Utilities -----------------
@dataclass
class Chunk:
    id: str
    text: str
    source: str
    chunk_id: int

def read_text_from_file(path: str) -> str:
    if path.lower().endswith(".pdf"):
        reader = PdfReader(path)
        return "\n\n".join(page.extract_text() or "" for page in reader.pages)
    elif path.lower().endswith((".txt", ".md", ".markdown")):
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            return f.read()
    else:
        return ""

def simple_chunk(text: str, max_tokens: int = 350, overlap: int = 50) -> List[str]:
    # Token-agnostic simple splitter by words. Keeps it robust without extra deps.
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        end = min(len(words), i + max_tokens)
        chunk_words = words[i:end]
        chunks.append(" ".join(chunk_words))
        if end == len(words):
            break
        i = max(0, end - overlap)
    return [c.strip() for c in chunks if c.strip()]

def build_or_get_collection(embedder: SentenceTransformer):
    client = chromadb.PersistentClient(path=PERSIST_DIR, settings=Settings(allow_reset=True))

    # Create or get collection
    colls = [c.name for c in client.list_collections()]
    if COLLECTION_NAME not in colls:
        collection = client.create_collection(COLLECTION_NAME, metadata={"hnsw:space": "cosine"})
    else:
        collection = client.get_collection(COLLECTION_NAME)

    return client, collection

def rebuild_index(embedder: SentenceTransformer, kb_dir: str = "data") -> int:
    client, collection = build_or_get_collection(embedder)
    # Reset collection
    try:
        client.delete_collection(COLLECTION_NAME)
    except Exception:
        pass
    collection = client.create_collection(COLLECTION_NAME, metadata={"hnsw:space": "cosine"})

    files = []
    if os.path.isdir(kb_dir):
        for name in os.listdir(kb_dir):
            if name.lower().endswith((".pdf", ".txt", ".md", ".markdown")):
                files.append(os.path.join(kb_dir, name))
    count = 0

    for f in files:
        raw = read_text_from_file(f)
        for idx, ch in enumerate(simple_chunk(raw)):
            chunk_id = str(uuid.uuid4())
            emb = embedder.encode([ch])[0].tolist()
            metadata = {"source": os.path.basename(f), "chunk": idx}
            collection.add(ids=[chunk_id], embeddings=[emb], documents=[ch], metadatas=[metadata])
            count += 1
    return count

def retrieve(query: str, embedder: SentenceTransformer, k: int = 4) -> List[Dict[str, Any]]:
    _, collection = build_or_get_collection(embedder)
    q_emb = embedder.encode([query]).tolist()
    res = collection.query(query_embeddings=q_emb, n_results=k)
    out = []
    for i in range(len(res["ids"][0])):
        out.append({
            "id": res["ids"][0][i],
            "text": res["documents"][0][i],
            "metadata": res["metadatas"][0][i],
            "distance": res["distances"][0][i] if "distances" in res else None,
        })
    return out

# -------------- Sentiment & Emotion --------------
@st.cache_resource(show_spinner=False)
def load_sentiment_pipeline():
    return pipeline("sentiment-analysis", model=SENTIMENT_MODEL)


@st.cache_resource(show_spinner=False)
def load_emotion_pipeline():
    try:
        return pipeline("text-classification", model=EMOTION_MODEL, return_all_scores=False)
    except Exception:
        return None

def analyze_mood(text: str) -> Dict[str, Any]:
    sent_pipe = load_sentiment_pipeline()
    sent = sent_pipe(text[:512])[0]  # keep it fast
    label = sent["label"]
    score = float(sent["score"])

    emo_pipe = load_emotion_pipeline()
    emotion = None
    if emo_pipe:
        try:
            res = emo_pipe(text[:512])[0]
            emotion = {"label": res["label"], "score": float(res.get("score", 0.0))}
        except Exception:
            emotion = None
    return {"sentiment": {"label": label, "score": score}, "emotion": emotion}

NEGATIVE_TRIGGERS = {"angry", "outrage", "refund", "chargeback", "complaint", "escalate", "manager", "legal", "lawsuit", "cancel"}

def heuristic_escalation(chat_turns: List[Dict[str, Any]], latest_mood: Dict[str, Any]) -> Tuple[float, str]:
    """Return risk in [0,1] and reason."""
    # Base risk from latest sentiment
    risk = 0.2
    reason = []
    if latest_mood["sentiment"]["label"].upper() == "NEGATIVE":
        risk += 0.4
        reason.append("negative sentiment")
    # Trend: count last 3 turns sentiment
    last_user = [t for t in chat_turns if t["role"] == "user"][-3:]
    if len(last_user) >= 2:
        negs = sum(1 for t in last_user if t.get("mood", {}).get("sentiment", {}).get("label", "").upper() == "NEGATIVE")
        if negs >= 2:
            risk += 0.2
            reason.append("downward trend")
    # Keywords
    text = (chat_turns[-1]["content"] if chat_turns else "").lower()
    if any(w in text for w in NEGATIVE_TRIGGERS):
        risk += 0.2
        reason.append("escalation keywords")
    return max(0.0, min(1.0, risk)), ", ".join(reason) if reason else "baseline risk"

# Optional: simple trainable model if labeled CSV exists
MODEL_PATH = "./escalation_clf.joblib"


def try_train_escalation_model(csv_path: str) -> str:
    import joblib
    import pandas as pd
    df = pd.read_csv(csv_path)
    # expects columns: text, escalated (0/1)
    X_train, X_test, y_train, y_test = train_test_split(df["text"], df["escalated"], test_size=0.2, random_state=42)
    clf = SkPipeline([
        ("tfidf", TfidfVectorizer(max_features=20000, ngram_range=(1,2))),
        ("logreg", LogisticRegression(max_iter=200))
    ])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    report = classification_report(y_test, y_pred)
    joblib.dump(clf, MODEL_PATH)
    return report

def try_predict_escalation_with_model(text: str) -> Tuple[float, float]:
    import joblib
    if not os.path.exists(MODEL_PATH):
        return None
    clf = joblib.load(MODEL_PATH)
    proba = clf.predict_proba([text])[0]
    # proba[1] = escalate probability
    return float(proba[1]), float(proba[0])

# -------------- Generator --------------
SYS_PROMPT = f"""
You are an empathetic {DEFAULT_DOMAIN} customer support agent.
Use the provided context from help articles to answer briefly (under 130 words),
first acknowledge the user's emotion, then provide a clear step-by-step fix, and end with a reassurance.
If context is missing, say you'll check and offer a safe workaround, without inventing facts.
Keep the tone: calm, respectful, concise.
"""


def format_context(docs: List[Dict[str, Any]]) -> str:
    formatted = []
    for d in docs:
        src = d["metadata"].get("source", "unknown")
        chunk = d["metadata"].get("chunk", -1)
        formatted.append(f"[From {src}#{chunk}] {d['text']}")
    return "\n\n".join(formatted)

def empathetic_template_reply(user_msg: str, mood: Dict[str, Any], ctx: List[Dict[str, Any]]):
    sent = mood["sentiment"]["label"].lower()
    emo = mood.get("emotion", {}).get("label", "") if mood.get("emotion") else ""
    lead = {
        "positive": "Great question, and thanks for reaching out!",
        "negative": "I hear your frustration and I’m here to help.",
        "neutral": "Thanks for sharing the details — I’m here to help.",
    }.get(sent, "Thanks for reaching out — I’m here to help.")
    if emo:
        lead += f" I understand you may be feeling {emo.lower()}."

    bullet = "\n- "
    steps = []
    if ctx:
        # derive 2–3 action lines from context first sentences
        for d in ctx[:3]:
            first_sentence = d["text"].split(".")[0][:160]
            steps.append(first_sentence.strip(" -•"))
    else:
        steps = ["Open the app’s Help > Contact Support.", "Share the error message and your last 3 steps.", "We’ll get back within one business day."]

    closing = "If anything still doesn’t work, I can escalate this and stay with you until it’s resolved."
    body = f"{lead}\nHere’s what to do:{bullet + bullet.join(steps)}\n\n{closing}"
    return body

def call_openai(messages: List[Dict[str, str]]):
    # Lightweight OpenAI Chat Completions call without extra deps
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        return None
    import requests
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {
        "model": "gpt-4o-mini",
        "messages": messages,
        "temperature": 0.3,
        "max_tokens": 240,
    }
    try:
        r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=30)
        r.raise_for_status()
        data = r.json()
        return data["choices"][0]["message"]["content"]
    except Exception as e:
        return None

# -------------- Streamlit UI --------------
st.set_page_config(page_title=APP_NAME, layout="wide")
st.title(APP_NAME)

with st.sidebar:
    st.markdown("### Setup")
    st.write("Domain:", DEFAULT_DOMAIN)
    st.write("Embedding:", EMBEDDING_MODEL)
    st.write("Sentiment:", SENTIMENT_MODEL)
    st.write("Emotion:", EMOTION_MODEL)

    # Upload knowledge base
    st.markdown("**Upload help articles (PDF/TXT/MD)**")
    os.makedirs("data", exist_ok=True)
    uploads = st.file_uploader("Add/replace KB files", type=["pdf", "txt", "md", "markdown"], accept_multiple_files=True)
    if uploads:
        for f in uploads:
            with open(os.path.join("data", f.name), "wb") as out:
                out.write(f.read())
        st.success(f"Saved {len(uploads)} file(s) to ./data")

    st.markdown("---")
    if st.button("(Re)build Index", type="primary"):
        with st.spinner("Embedding & indexing …"):
            embedder = SentenceTransformer(EMBEDDING_MODEL)
            n = rebuild_index(embedder)
        st.success(f"Indexed {n} chunks.")

    st.markdown("---")
    st.markdown("**Optional: Train escalation model**")
    csv = st.file_uploader("Labeled CSV (columns: text, escalated)", type=["csv"], accept_multiple_files=False)
    if csv is not None:
        tmp_csv = f"/tmp/escalation_{uuid.uuid4().hex}.csv"
        with open(tmp_csv, "wb") as f:
            f.write(csv.read())
        with st.spinner("Training logistic regression …"):
            report = try_train_escalation_model(tmp_csv)
        st.text("Validation report:\n" + report)

# Session state
if "history" not in st.session_state:
    st.session_state.history = []  # list of {role, content, mood}

if "embedder" not in st.session_state:
    st.session_state.embedder = SentenceTransformer(EMBEDDING_MODEL)

# Chat box
st.markdown("### Chat")
for turn in st.session_state.history:
    with st.chat_message(turn["role"]):
        st.write(turn["content"])
        if turn["role"] == "user" and turn.get("mood"):
            m = turn["mood"]
            st.caption(f"Sentiment: {m['sentiment']['label']} ({m['sentiment']['score']:.2f})" + (f" · Emotion: {m['emotion']['label']}" if m.get('emotion') else ""))

user_msg = st.chat_input("Type your support message…")

if user_msg:
    # Analyze mood
    mood = analyze_mood(user_msg)

    # Retrieve context
    t0 = time.time()
    docs = retrieve(user_msg, st.session_state.embedder, k=4)
    latency_ms = (time.time() - t0) * 1000

    # Escalation risk
    hist = st.session_state.history + [{"role": "user", "content": user_msg, "mood": mood}]
    model_risk = try_predict_escalation_with_model(user_msg)
    if model_risk:
        risk, _ = model_risk[0], model_risk[1]
        reason = "trained model probability"
    else:
        risk, reason = heuristic_escalation(hist, mood)

    # Generate response
    ctx_text = format_context(docs)
    sys = {"role": "system", "content": SYS_PROMPT}
    usr = {"role": "user", "content": f"User message: {user_msg}\n\nContext to use:\n{ctx_text}"}
    ai = call_openai([sys, usr])
    if not ai:
        ai = empathetic_template_reply(user_msg, mood, docs)

    # Log chat
    st.session_state.history.append({"role": "user", "content": user_msg, "mood": mood})
    st.session_state.history.append({"role": "assistant", "content": ai})

    # Display assistant message with insights
    with st.chat_message("assistant"):
        st.write(ai)
        st.progress(min(1.0, risk))
        st.caption(f"Escalation risk: {risk:.2f} · Reason: {reason} · Retrieval latency: {latency_ms:.0f} ms")
        if docs:
            with st.expander("Sources (top matches)"):
                for i, d in enumerate(docs, 1):
                    st.markdown(f"**{i}.** `{d['metadata'].get('source','')}` · chunk #{d['metadata'].get('chunk','')}")
                    st.write(d['text'][:500] + ("…" if len(d['text']) > 500 else ""))

# --- Simple evaluation tab ---
st.markdown("---")
st.subheader("Evaluation")
col1, col2 = st.columns(2)
with col1:
    st.markdown("**Quick Retrieval Accuracy (Hit@k)**")
    st.write("Upload a small JSONL with items: {'query': str, 'expected_source': str}")
    eval_file = st.file_uploader("JSONL eval set", type=["jsonl"], key="eval")
    if eval_file:
        lines = [json.loads(l) for l in eval_file.read().decode("utf-8").splitlines() if l.strip()]
        hits = 0
        for item in lines:
            res = retrieve(item["query"], st.session_state.embedder, k=3)
            top_sources = {r["metadata"].get("source", "") for r in res}
            if item["expected_source"] in top_sources:
                hits += 1
        st.success(f"Hit@3: {hits}/{len(lines)} = {hits/len(lines):.2f}")

with col2:
    st.markdown("**CSAT Tracking (demo)**")
    if st.session_state.history:
        latest = st.session_state.history[-1]["content"] if st.session_state.history[-1]["role"] == "assistant" else None
        score = st.slider("How satisfied are you with the last reply?", 1, 5, 4)
        if st.button("Log CSAT"):
            log = {"ts": time.time(), "score": score, "assistant": latest}
            os.makedirs(".logs", exist_ok=True)
            with open(".logs/csat.jsonl", "a") as f:
                f.write(json.dumps(log) + "\n")
            st.toast("Saved CSAT.")

st.markdown(
    """
---
**Deployment tips**
- Works on Hugging Face Spaces (CPU). Add `requirements.txt` with the pip line shown at top.
- To enable OpenAI responses, set a secret `OPENAI_API_KEY`.
- Persisted Chroma DB is stored in `/home/user/app/chroma_db` by default on Spaces.
- Add a few sample KB files under `/data` in the repo for immediate demo.
"""
)

