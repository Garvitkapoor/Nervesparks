# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lgz2R-e9JqPzooYgPyf3A8Dn2vO0Be83
"""

# app.py
"""
Customer Support RAG w/ Sentiment & Escalation — single-file Streamlit app

How to run (Colab / local):
1) Save this file as app.py
2) In Colab: !streamlit run app.py  (may require ngrok for external access)
   Locally: streamlit run app.py
3) Optional: set OPENAI_API_KEY in sidebar to enable GPT-based generation

Features:
- Upload help articles (txt/csv/pdf), chunk & embed with sentence-transformers
- Use Chroma (in-memory) as vector DB
- Retrieve top-k relevant passages for a query
- Sentiment analysis + emotion detection (transformers pipelines)
- Escalation risk: heuristic and trainable model (if user provides labeled CSV)
- Empathetic response generation: OpenAI (preferred) or fallback template / HF generator
- Stores interaction logs and simple satisfaction tracking (downloadable CSV)
"""

# --- Auto-install dependencies if missing (works in Colab/local) ---
import subprocess, sys, os, json, io
def pip_install(pkgs):
    subprocess.check_call([sys.executable, "-m", "pip", "install", "--quiet"] + pkgs)
required = [
    "streamlit", "sentence-transformers", "chromadb", "transformers", "torch",
    "scikit-learn", "PyPDF2", "pandas", "nltk", "tqdm", "openai"
]
try:
    import streamlit as st
except Exception:
    pip_install(required)
finally:
    import streamlit as st

# --- Imports ---
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.utils import embedding_functions
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import PyPDF2
import nltk
from nltk.tokenize import sent_tokenize
from tqdm.auto import tqdm
import openai
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, TextGenerationPipeline, set_seed
import time
import tempfile
from io import BytesIO

# Ensure nltk punkt
nltk.download('punkt', quiet=True)

# --- Helper functions for file processing ---
def extract_text_from_pdf(file_bytes):
    reader = PyPDF2.PdfReader(io.BytesIO(file_bytes))
    pages = []
    for p in range(len(reader.pages)):
        try:
            pages.append(reader.pages[p].extract_text() or "")
        except Exception:
            continue
    return "\n".join(pages)

def load_kb_file(uploaded_file):
    # returns text content
    name = uploaded_file.name.lower()
    b = uploaded_file.read()
    if name.endswith(".pdf"):
        return extract_text_from_pdf(b)
    elif name.endswith(".txt"):
        return b.decode("utf-8", errors="ignore")
    elif name.endswith(".csv"):
        # assume 'content' or join text columns
        df = pd.read_csv(io.BytesIO(b))
        # heuristic: concat text-like columns
        text_cols = [c for c in df.columns if df[c].dtype == object]
        if len(text_cols) == 0:
            return df.to_csv(index=False)
        combined = df[text_cols].astype(str).agg(" ".join, axis=1).str.cat(sep="\n")
        return combined
    else:
        # fallback
        try:
            return b.decode("utf-8", errors="ignore")
        except:
            return ""

# Chunking strategy: sentence-based with max_chars per chunk
def chunk_text(text, max_chars=800, overlap=100):
    sentences = sent_tokenize(text)
    chunks = []
    cur = ""
    for s in sentences:
        if len(cur) + len(s) + 1 <= max_chars:
            cur = cur + " " + s if cur else s
        else:
            if cur:
                chunks.append(cur.strip())
            # if sentence itself too long, break it
            if len(s) > max_chars:
                for i in range(0, len(s), max_chars - 50):
                    chunks.append(s[i:i+max_chars])
                cur = ""
            else:
                cur = s
    if cur:
        chunks.append(cur.strip())
    # apply overlap by merging last n chars to next chunk if small
    if overlap and len(chunks) > 1:
        new_chunks = []
        for i, c in enumerate(chunks):
            if i == 0:
                new_chunks.append(c)
            else:
                # attach last overlap chars of previous
                prev = new_chunks[-1]
                back = prev[-overlap:] if len(prev) > overlap else prev
                new_chunks.append((back + " " + c).strip())
        chunks = new_chunks
    return chunks

# --- Embedding & Vector DB setup ---
@st.cache_resource(show_spinner=False)
def load_embed_model(name="all-MiniLM-L6-v2"):
    return SentenceTransformer(name)

# We will use Chroma in-process (no external Pinecone)
def init_chroma_client(persist_directory=None):
    # Uses chromadb default in-memory; for persistence user can set persist_directory
    client = chromadb.Client()
    return client

# Build or re-build a collection with KB
def build_collection(client, collection_name, texts, metadatas, embed_model):
    # Create or reset collection
    try:
        client.delete_collection(name=collection_name)
    except Exception:
        pass
    # Use SentenceTransformer embeddings (we'll compute embeddings ourselves)
    collection = client.create_collection(name=collection_name)
    # compute embeddings in batches
    batch_size = 64
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        emb = embed_model.encode(batch, show_progress_bar=False, convert_to_numpy=True)
        embeddings.append(emb)
    embeddings = np.vstack(embeddings)
    ids = [f"doc_{i}" for i in range(len(texts))]
    collection.add(
        ids=ids,
        metadatas=metadatas,
        documents=texts,
        embeddings=embeddings.tolist()
    )
    return collection

# Retrieve top_k
def retrieve(collection, query, embed_model, top_k=5):
    q_emb = embed_model.encode([query], convert_to_numpy=True)[0].tolist()
    res = collection.query(
        query_embeddings=[q_emb],
        n_results=top_k,
        include=['documents','metadatas','distances']
    )
    # structure results
    docs = []
    if res and 'documents' in res and len(res['documents'])>0:
        for doc, meta, dist in zip(res['documents'][0], res['metadatas'][0], res['distances'][0]):
            docs.append({"document": doc, "meta": meta, "distance": dist})
    return docs

# --- Sentiment & Emotion pipelines (HF) ---
@st.cache_resource(show_spinner=False)
def get_sentiment_pipeline():
    # default sentiment pipeline (distilbert-based)
    try:
        pipe = pipeline("sentiment-analysis", top_k=None)
    except Exception:
        # fallback
        pipe = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
    return pipe

@st.cache_resource(show_spinner=False)
def get_emotion_pipeline():
    # emotion detection
    try:
        # model with 'emotion' label set
        pipe = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", return_all_scores=True)
    except Exception:
        # fallback
        pipe = pipeline("text-classification", model="nateraw/bert-base-uncased-emotion", return_all_scores=True)
    return pipe

# compute sentiment + emotion summary
def analyze_mood(text, sentiment_pipe, emotion_pipe):
    s = sentiment_pipe(text[:512])[0]  # huggingface returns dict e.g. {'label':'POSITIVE','score':0.99}
    e_scores = emotion_pipe(text[:512])[0]  # list of label-score
    # choose top emotion
    top_e = max(e_scores, key=lambda x: x['score'])
    return {"sentiment_label": s['label'], "sentiment_score": float(s['score']),
            "emotion": top_e['label'], "emotion_score": float(top_e['score']),
            "all_emotions": e_scores}

# --- Escalation prediction: heuristic + trainable model ---
def heuristic_escalation_check(text, sentiment_label, sentiment_score, emotion, history_count=1):
    # rules:
    # - very negative sentiment (NEGATIVE with high score)
    # - mention of keywords
    # - repeated contact hint in text (user mentions 'again', 'still', 'not resolved')
    keywords = ["manager","refund","escalate","supervisor","complain","not resolved","still not","again","urgent","lawsuit","legal"]
    text_l = text.lower()
    key_flag = any(k in text_l for k in keywords)
    neg_flag = (sentiment_label.upper() in ["NEGATIVE","NEG"]) and (sentiment_score >= 0.6)
    repeats = any(w in text_l for w in ["again","still","not resolved","still not","speak to"])
    if (neg_flag and key_flag) or repeats or sentiment_score<0.35 and sentiment_label.upper() in ["NEGATIVE","NEG"]:
        return 0.85  # high risk
    if neg_flag:
        return 0.6
    if key_flag:
        return 0.5
    return 0.1

# Trainable escalation predictor (user can upload labeled CSV with 'text' and 'escalated' columns)
def train_escalation_model(df, embed_model):
    # df: must have 'text' and 'escalated' (0/1)
    X = embed_model.encode(df['text'].tolist(), convert_to_numpy=True)
    y = df['escalated'].astype(int).values
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y))>1 else None)
    clf = LogisticRegression(max_iter=1000)
    clf.fit(X_train, y_train)
    preds = clf.predict(X_val)
    report = classification_report(y_val, preds, output_dict=True, zero_division=0)
    return clf, report

# --- Response generation ---
def generate_response_with_openai(api_key, system_prompt, user_prompt, retrieved_context, temperature=0.2, max_tokens=300):
    openai.api_key = api_key
    prompt = system_prompt + "\n\nContext:\n" + retrieved_context + "\n\nUser:\n" + user_prompt + "\n\nInstruction: Provide an empathetic, concise, actionable customer support reply. If escalation is needed, mention steps and recommend escalation politely."
    try:
        resp = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role":"system","content":system_prompt},{"role":"user","content": "Context:\n" + retrieved_context + "\n\n" + user_prompt}],
            temperature=temperature,
            max_tokens=max_tokens,
            n=1
        )
        text = resp['choices'][0]['message']['content'].strip()
        return text
    except Exception as e:
        return f"(OpenAI error: {e})\n\nPlease either set a valid OpenAI key or use the fallback generator."

# fallback generator: templated response using retrieved KB passages and sentiment/emotion
def fallback_generate(user_text, retrieved_docs, sentiment, emotion, escalation_risk):
    # Build context summary
    ctx = "\n\n".join([f"- {d['document'][:300]}..." for d in (retrieved_docs or [])][:3]) if retrieved_docs else "No relevant KB found."
    tone = "very empathetic" if sentiment['sentiment_label'].lower().startswith("neg") or emotion.lower() in ["anger","sadness"] else "professional and friendly"
    escalate_msg = ""
    if escalation_risk > 0.7:
        escalate_msg = "\n\n⚠️ This conversation looks high-risk for escalation. Please offer a direct escalation path (manager/senior support) and a goodwill remedy if appropriate."

    # Safely get first suggested action without using backslashes inside f-string expressions
    if retrieved_docs:
        # use splitlines() instead of split('\\n') to avoid backslashes in f-string expressions
        first_action = retrieved_docs[0]['document'][:300].splitlines()[0] if retrieved_docs[0]['document'].strip() else "Please provide more details so I can help."
    else:
        first_action = "Please provide more details so I can help."

    # Compose reply (no backslashes inside { ... } parts)
    reply = (
        "Hello — I’m sorry you’re facing this (I understand this can be frustrating).\n\n"
        f"I've checked relevant help documents and found:\n{ctx}\n\n"
        "Suggested action (concise):\n"
        f"1. {first_action}\n"
        "2. If the above doesn't work, try contacting support with your order/reference number.\n\n"
        f"Tone: {tone}.{escalate_msg}\n\n"
        "If you'd like, I can escalate this for you — just confirm and provide any order/reference details."
    )
    return reply


# Optional light HF generator when no OpenAI: use small causal LM (gpt2) — limited capability
@st.cache_resource(show_spinner=False)
def get_hf_generator(model_name="gpt2"):
    try:
        tok = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        gen = TextGenerationPipeline(model=model, tokenizer=tok, device=-1)
        set_seed(42)
        return gen
    except Exception:
        return None

# --- Interaction logging ---
@st.cache_resource
def get_log_df():
    return pd.DataFrame(columns=["timestamp","user_text","response","sentiment_label","sentiment_score","emotion","emotion_score","escalation_risk","satisfaction"])

# --- Streamlit UI ---
st.set_page_config(page_title="RAG Customer Support Demo", layout="wide")
st.title("Customer Support RAG with Sentiment & Escalation — Single-file Demo")

# Sidebar: controls
st.sidebar.header("System Settings")
embed_model_name = st.sidebar.selectbox("Embedding model (sentence-transformers)", ["all-MiniLM-L6-v2","all-mpnet-base-v2"], index=0)
collection_name = st.sidebar.text_input("Vector collection name", value="kb_collection")
openai_key = st.sidebar.text_input("OpenAI API key (optional)", type="password")
if openai_key:
    st.sidebar.success("OpenAI key set — will use GPT for generation")
use_chroma_persist = st.sidebar.checkbox("Persist Chroma locally (saves to /tmp/chroma)", value=False)
chroma_path = "/tmp/chroma_db" if use_chroma_persist else None

# Upload KB files
st.sidebar.markdown("---")
st.sidebar.subheader("Knowledge Base")
kb_files = st.sidebar.file_uploader("Upload help articles (txt/csv/pdf). You can upload multiple.", accept_multiple_files=True, type=['txt','csv','pdf'])
max_chars = st.sidebar.slider("Chunk size (chars)", min_value=400, max_value=2000, value=800, step=100)
overlap = st.sidebar.slider("Chunk overlap (chars)", min_value=0, max_value=400, value=100, step=50)
top_k = st.sidebar.slider("Retrieval top-k", 1, 10, 5)
rebuild_index_btn = st.sidebar.button("Build / Rebuild Index")

# Optional: upload escalation labeled CSV to train model
st.sidebar.markdown("---")
st.sidebar.subheader("Escalation Predictor (optional)")
escalation_csv = st.sidebar.file_uploader("Upload labeled escalation CSV (columns: text, escalated 0/1)", type=['csv'])
train_escalation = st.sidebar.button("Train Escalation Model")

# Main area: KB contents preview & indexing
st.header("Knowledge Base & Index")
if 'chroma_client' not in st.session_state:
    st.session_state['chroma_client'] = init_chroma_client(persist_directory=chroma_path)

embed_model = load_embed_model(embed_model_name)
sentiment_pipe = get_sentiment_pipeline()
emotion_pipe = get_emotion_pipeline()
hf_gen = get_hf_generator()  # may be None

if 'collection' not in st.session_state:
    st.session_state['collection'] = None
    st.session_state['kb_texts'] = []
    st.session_state['kb_metas'] = []

# Process uploaded KB files (not auto-build index until button clicked)
if kb_files:
    all_texts = []
    metas = []
    for f in kb_files:
        txt = load_kb_file(f)
        # chunk
        chunks = chunk_text(txt, max_chars=max_chars, overlap=overlap)
        for i,c in enumerate(chunks):
            all_texts.append(c)
            metas.append({"source": f.name, "chunk_id": i})
    st.session_state['kb_texts'] = all_texts
    st.session_state['kb_metas'] = metas
    st.write(f"Loaded {len(all_texts)} chunks from {len(kb_files)} file(s). Preview first 3 chunks:")
    for i,chunk in enumerate(all_texts[:3]):
        st.markdown(f"**Chunk {i+1}:** _from {metas[i]['source']}_\n\n{chunk[:1000]}")

# Build index button
if rebuild_index_btn:
    if not st.session_state.get('kb_texts'):
        st.warning("No KB uploaded. Upload files in the sidebar first.")
    else:
        with st.spinner("Building vector collection..."):
            st.session_state['collection'] = build_collection(st.session_state['chroma_client'], collection_name, st.session_state['kb_texts'], st.session_state['kb_metas'], embed_model)
        st.success("Index built and stored in Chroma collection.")

# Train escalation model
if train_escalation:
    if escalation_csv is None:
        st.warning("Upload a labeled CSV first.")
    else:
        df = pd.read_csv(escalation_csv)
        if 'text' not in df.columns or 'escalated' not in df.columns:
            st.error("CSV must contain 'text' and 'escalated' columns.")
        else:
            with st.spinner("Training escalation model..."):
                clf, report = train_escalation_model(df, embed_model)
                st.session_state['escalation_clf'] = clf
                st.session_state['escalation_report'] = report
            st.success("Trained escalation model. Validation report:")
            st.json(report)

# Chat / Query area
st.header("Chat & Retrieval")
if st.session_state.get('collection') is None:
    st.info("No index found. Upload KB and click 'Build / Rebuild Index' in sidebar to create the vector collection.")
query = st.text_area("User message (example: 'My refund hasn't arrived and support isn't helping')", height=120)
col1, col2 = st.columns([2,1])
with col2:
    st.markdown("### Quick Controls")
    use_openai = st.checkbox("Use OpenAI for generation (if key set)", value=bool(openai_key))
    gen_temp = st.slider("Generation temperature", 0.0, 1.0, 0.2)
    send_btn = st.button("Send / Retrieve & Respond")

# Session for logs
if 'interaction_log' not in st.session_state:
    st.session_state['interaction_log'] = get_log_df()

# On send: run retrieval, sentiment, escalation, and generate response
if send_btn and query.strip():
    if st.session_state.get('collection') is None:
        st.error("No KB index — cannot retrieve. Build index first.")
    else:
        with st.spinner("Retrieving relevant docs & analyzing sentiment..."):
            retrieved = retrieve(st.session_state['collection'], query, embed_model, top_k=top_k)
            # Sentiment & emotion
            mood = analyze_mood(query, sentiment_pipe, emotion_pipe)
            # Heuristic escalation
            esc_risk = heuristic_escalation_check(query, mood['sentiment_label'], mood['sentiment_score'], mood['emotion'])
            # If trained clf available, use it
            if st.session_state.get('escalation_clf') is not None:
                emb_q = embed_model.encode([query], convert_to_numpy=True)
                prob = st.session_state['escalation_clf'].predict_proba(emb_q)[0][1]
                esc_risk = float(prob)
        # Show retrieval
        st.subheader("Top retrieved passages")
        for i,doc in enumerate(retrieved):
            st.markdown(f"**[{i+1}] Source:** {doc['meta'].get('source','unknown')} — distance: {doc['distance']:.4f}")
            st.write(doc['document'][:800] + ("..." if len(doc['document'])>800 else ""))
        # Show mood & escalation
        st.write("---")
        st.markdown("**Sentiment & Emotion**")
        st.write(f"- Sentiment: **{mood['sentiment_label']}** ({mood['sentiment_score']:.2f})")
        st.write(f"- Emotion (top): **{mood['emotion']}** ({mood['emotion_score']:.2f})")
        st.write(f"- Escalation risk (0-1): **{esc_risk:.2f}**")
        st.write("---")

        # Generate response
        retrieved_context = "\n\n".join([d['document'] for d in retrieved[:5]])
        system_prompt = "You are a helpful, empathetic customer support assistant. Keep replies concise, action-oriented, and polite. When escalation is recommended, instruct next steps and apologize."
        # Prefer OpenAI if key and checkbox set
        reply_text = None
        if use_openai and openai_key:
            try:
                reply_text = generate_response_with_openai(openai_key, system_prompt, query, retrieved_context, temperature=gen_temp)
            except Exception as e:
                reply_text = f"(OpenAI generation failed: {e})"
        # fallback to HF generator if available
        if not reply_text or reply_text.strip()=="":
            # attempt hf generator
            if hf_gen is not None:
                prompt = f"{system_prompt}\n\nContext:\n{retrieved_context}\n\nUser: {query}\n\nResponse:"
                try:
                    out = hf_gen(prompt, max_length=200, do_sample=True, min_length=50, num_return_sequences=1)
                    reply_text = out[0]['generated_text'].strip()
                except Exception:
                    reply_text = fallback_generate(query, retrieved, {"sentiment_label":mood['sentiment_label'],"sentiment_score":mood['sentiment_score']}, mood['emotion'], esc_risk)
            else:
                reply_text = fallback_generate(query, retrieved, {"sentiment_label":mood['sentiment_label'],"sentiment_score":mood['sentiment_score']}, mood['emotion'], esc_risk)

        st.subheader("Assistant response")
        st.write(reply_text)

        # Log interaction
        log_row = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "user_text": query,
            "response": reply_text,
            "sentiment_label": mood['sentiment_label'],
            "sentiment_score": mood['sentiment_score'],
            "emotion": mood['emotion'],
            "emotion_score": mood['emotion_score'],
            "escalation_risk": esc_risk,
            "satisfaction": None
        }
        st.session_state['interaction_log'] = pd.concat([st.session_state['interaction_log'], pd.DataFrame([log_row])], ignore_index=True)

        # Satisfaction feedback
        st.write("---")
        st.markdown("### Customer satisfaction")
        sat = st.radio("Rate the helpfulness of the reply", options=["Very satisfied","Satisfied","Neutral","Dissatisfied","Very dissatisfied"], index=2, horizontal=False)
        if st.button("Submit rating"):
            st.session_state['interaction_log'].loc[st.session_state['interaction_log'].index[-1],'satisfaction'] = sat
            st.success("Rating saved.")

# Interaction logs & export
st.header("Interaction Logs & Metrics")
st.write("Recent interactions (saved in-session):")
st.dataframe(st.session_state['interaction_log'].tail(10))
csv_buffer = io.StringIO()
st.session_state['interaction_log'].to_csv(csv_buffer, index=False)
st.download_button("Download logs CSV", csv_buffer.getvalue(), file_name="interaction_logs.csv", mime="text/csv")

# Simple metrics
if len(st.session_state['interaction_log'])>0:
    st.subheader("Simple metrics")
    df_logs = st.session_state['interaction_log']
    satis_map = {"Very satisfied":5,"Satisfied":4,"Neutral":3,"Dissatisfied":2,"Very dissatisfied":1}
    df_logs['sati_score'] = df_logs['satisfaction'].map(satis_map)
    avg_satis = float(df_logs['sati_score'].dropna().mean()) if df_logs['sati_score'].notna().any() else None
    st.write(f"- Total interactions: {len(df_logs)}")
    if avg_satis:
        st.write(f"- Average satisfaction score: {avg_satis:.2f} (1-5)")
    # Escalation distribution
    st.write("- Escalation risk distribution (simple):")
    st.bar_chart(df_logs['escalation_risk'].fillna(0).astype(float).rolling(1).mean())

st.markdown("---")
st.caption("Notes: This demo is for educational use. For production systems: secure keys, persistent vector DB (Chroma/Pinecone), rate limits, privacy & consent, and more sophisticated LLMs and training for escalation prediction are required.")

